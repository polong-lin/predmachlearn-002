Predmachlearn-002: Prediction Assignment Writeup
========================================================
By: Polong Lin

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, my goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

## Goal: Predicting which of the five exercises was performed
The goal of this project is to predict the manner in which participants performed the exercise. This is the "classe" variable in the training set. Any of the other variables are available to predict with. In the end, I will also use my prediction model to predict 20 different test cases. 

## Loading the dataset

```{r, cache = TRUE}
library(caret); library(randomForest)
train <- read.csv("pml-training.csv", stringsAsFactors = FALSE)
test <- read.csv("pml-testing.csv", stringsAsFactors = FALSE)
```

## Pre-processing
Many variables contained mostly NA's or strings of characters. Those were removed after manual inspection, resulting in 54 remaining variables. There were many observations of each variable (n = 19622). Because random forest processing can be quite time-consuming, I subsetted only 5000 rows for the classification to process more quickly.

```{r, cache=TRUE}
set.seed(13243)

#Sample 3000 rows out of original training data
train.sample <- sample(nrow(train), 5000) 
train.subset <- train[train.sample,]

#Remove irrelevant, unnecessary columns
train.dropna <- train.subset[,colSums(is.na(train.subset)) < 100] #Columns with >100 NA values
train.dropchr <- train.dropna[, !sapply(train.dropna, is.character)] #Columns of "character"" type
train.final <- train.dropchr[,4:56] #Remove other irrelevant columns through manual inspection
train.final$classe <- factor(train.subset$classe) #Re-insert classe column, convert to factor

#createDataPartition: Subset 20% of training data for cross-validation
trainIndex <- createDataPartition(y = train.final$classe, p=0.8, list = FALSE) 
train1 <- train.final[trainIndex,]
train2 <- train.final[-trainIndex,]
```


## Running a Random Forest classification tree
Here, I run a random forest algorithm to train on 80% of the training data (`r dim(train1)[1]` observations). For cross-validation, I have reserved the remaining 20% (`r dim(train2)[1]` observations).  

Below, you can see the output for the final model, "modFit1".

```{r, cache=TRUE}
#Random forest on train1
modFit1 <- train(train1$classe ~., data = train1, method = "rf")

modFit1$finalModel
```

## Expectations for out-of-sample error
As indicated in the output for the model above, the OOB estimate of error rate was 1.37%. In-sample error estimates are almost always too optimistic, as the estimates may be incorporating the noise from the in-sample set. Out-sample estimates of error are typically larger than in-sample estimates. Thus, I expect that the out-sample error rate will be around 1.37%, but most likely higher than 1.37%. A much higher error rate would indicate that the model is overfitting, or that there is something very different between the training set and the cross-validation dataset.

## Cross-validation
Here, I test the random forest model, "modFit1" on the remaining partitioned data, "train2", that contains `r dim(train2)[1]` observations. We then report the confusion matrix showing how the model's predictions compared with the actual classes from the cross-validation dataset.

```{r, cache=TRUE}
predictions <- predict(modFit1, newdata = train2)

confusionMatrix(predictions, train2$classe)
```

## Estimating out-of-sample error
```{r, echo=FALSE, cache=TRUE}
predictions.accuracy <-confusionMatrix(predictions, train2$classe)$overall[["Accuracy"]]

error.outofsample <- 1 - predictions.accuracy #Out of sample error: using 1-Accuracy for cross-validation sample (0.2 of training set)
```
In the confusion matrix above, observe that the accuracy of the model is `r predictions.accuracy`. Subtracting this value from 1 gives us the error rate, `r error.outofsample`, or `r error.outofsample*100`%. Compared to the in-sample error rate of 1.37%, we can observe that the out-of-sample error rate of `r error.outofsample*100`% was actually slightly lower! This suggests that our prediction model is doing well - it is not overfitting the training dataset, and is able to predict a new dataset very well.

Overall, this model appears to have performed very well - 99% of the cross-validation dataset was classified correctly!

## Predictions of Test dataset
Below are my final predictions for the test set of 20 cases.
```{r, cache = TRUE}
predictions.test <- predict(modFit1, newdata = test)
data.frame("Predictions" = predictions.test)
```

